# Курсовая работа по иерархическуму планированию с дообучением
На данном этапе почти реализован фреймворк Options для среды GridWorld.
Оригинальный репозиторий: [[q-learning-with-options]](https://github.com/s-mawjee/q-learning-with-options)

# Dependencies
- Numpy
- Matplotlib
- Gym

# Запуск
Клонировать репозиторий и запустить train.py (в методе main() устанавливаются параметры запуска в переменной parameters).

# Вопросы и проблемы 
1) Не совсем понятно, стоит ли использовать R-learning (как в статье про PEORL) или же остановиться на Q-learning.
2) Также я не совсем понял, те самые опции (aka options) задаются изначально или надо агенту самому к ним прийти? Являются ли они детерминированными или внутри у них тоже существуют стратегии, которые обучаются?

